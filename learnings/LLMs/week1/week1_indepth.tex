% Defining the document class and setting up the page layout
\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{noto}

% Setting up fonts last, ensuring compatibility
\usepackage{noto}

% Title and author information
\title{Week 1 LLM Learning Plan: In-Depth Overview}
\author{Your Name}
\date{May 26--June 2, 2025}

\begin{document}

% Generating the title page
\maketitle

% Starting the introduction section
\section*{Introduction}
This document provides an in-depth yet brief overview of Week 1 (May 26--June 2, 2025) LLM learning, covering tokenization, word embeddings, RNNs, transformers, and fine-tuning. It includes advanced resources for revision, ideal for a full-stack web engineer to revisit concepts and deepen NLP expertise.

% Starting the topic overview section
\section*{Topic Overview}

% Tokenization
\subsection*{Tokenization}
\textbf{Overview}: Tokenization splits text into tokens (words, subwords) for numerical processing in LLMs. NLTK and BERT’s WordPiece handle different granularities. \\
\textbf{Key Insights}: Tokens convert to IDs; subword tokenization (e.g., ``LLMs'' to ``ll'' + ``##ms'') manages rare words. \\
\textbf{Week 1 Application}: Tokenized ``I love learning about LLMs'' (NLTK) and ``Transformers power modern LLMs'' (BERT). \\
\textbf{Advanced Resources}:
\begin{itemize}
    \item \href{https://huggingface.co/docs/transformers/tokenizer_summary}{Hugging Face Tokenizer Guide}: Details WordPiece, BPE.
    \item \href{https://arxiv.org/abs/1907.11692}{Subword Tokenization Paper}: Explores efficiency in LLMs.
\end{itemize}

% Word Embeddings
\subsection*{Word Embeddings}
\textbf{Overview}: Embeddings (e.g., Word2Vec, GloVe) map tokens to vectors, capturing semantic context. \\
\textbf{Key Insights}: Skip-gram predicts context; GloVe uses co-occurrence matrices. Cosine similarity measures relatedness (e.g., ``king'' vs. ``queen'': 0.7839). \\
\textbf{Week 1 Application}: Computed similarity with \texttt{gensim}’s GloVe. \\
\textbf{Advanced Resources}:
\begin{itemize}
    \item \href{https://arxiv.org/abs/1301.3781}{Word2Vec Paper}: Deep dive into Skip-gram/CBOW.
    \item \href{https://nlp.stanford.edu/projects/glove}{GloVe Project}: Co-occurrence mechanics.
\end{itemize}

% RNNs and LSTMs
\subsection*{RNNs and LSTMs}
\textbf{Overview}: RNNs process sequences sequentially; LSTMs mitigate vanishing gradients for longer contexts. \\
\textbf{Key Insights}: RNNs struggle with long dependencies; LSTMs use gates to retain memory. \\
\textbf{Week 1 Application}: Studied limitations vs. transformers’ parallel attention. \\
\textbf{Advanced Resources}:
\begin{itemize}
    \item \href{https://arxiv.org/abs/1406.1078}{LSTM Paper}: Original architecture details.
    \item \href{https://colah.github.io/posts/2015-09-NN-Types-FP}{Colah’s RNN Guide}: Advanced RNN variants.
\end{itemize}

% Transformers
\subsection*{Transformers}
\textbf{Overview}: Transformers use self-attention and FFNs for parallel, context-aware processing. \\
\textbf{Key Insights}: Attention weights token relationships; BERT’s encoder-only design excels in classification. \\
\textbf{Week 1 Application}: Explored attention, tokenized with BERT, ran sentiment analysis (score: 0.99983). \\
\textbf{Advanced Resources}:
\begin{itemize}
    \item \href{https://arxiv.org/abs/1706.03762}{Attention Is All You Need}: Transformer paper.
    \item \href{https://jalammar.github.io/illustrated-bert}{The Illustrated BERT}: Visualizes BERT’s flow.
\end{itemize}

% Fine-Tuning
\subsection*{Fine-Tuning}
\textbf{Overview}: Fine-tuning adapts pre-trained models (e.g., DistilBERT) to specific tasks like sentiment classification. \\
\textbf{Key Insights}: Hyperparameters (learning rate, epochs) impact accuracy. Validation loss monitors overfitting. \\
\textbf{Week 1 Application}: Fine-tuned DistilBERT on IMDB (92.4\% accuracy, later 87.7\% after tweaks). \\
\textbf{Advanced Resources}:
\begin{itemize}
    \item \href{https://huggingface.co/docs/transformers/training}{Hugging Face Fine-Tuning}: Advanced techniques.
    \item \href{https://arxiv.org/abs/1810.04805}{BERT Paper}: Fine-tuning strategies.
\end{itemize}

% Ending the document
\end{document}